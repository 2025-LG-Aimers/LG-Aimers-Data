import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import OrdinalEncoder
from sklearn.impute import SimpleImputer
from sklearn.metrics import roc_auc_score, accuracy_score

# -------------- ğŸ“Œ ë°ì´í„° ë¡œë”© --------------
train = pd.read_csv('C:/Users/ANTL/Documents/GitHub/LG-Aimers-Data/train.csv').drop(columns=['ID'])
test = pd.read_csv('C:/Users/ANTL/Documents/GitHub/LG-Aimers-Data/test.csv').drop(columns=['ID'])

# -------------- ğŸ“Œ íƒ€ê²Ÿ ë³€ìˆ˜ ë¶„ë¦¬ --------------
X = train.drop(columns=['ì„ì‹  ì„±ê³µ ì—¬ë¶€'])  # ì…ë ¥ ë°ì´í„° (Feature)
y = train['ì„ì‹  ì„±ê³µ ì—¬ë¶€']  # íƒ€ê²Ÿ ë³€ìˆ˜ (Label)

# âœ… í¸í–¥ëœ ì»¬ëŸ¼ ì œê±°
columns_to_remove = ["ì‹ ì„  ë°°ì•„ ì‚¬ìš© ì—¬ë¶€", "ë¯¸ì„¸ì£¼ì…ëœ ë‚œì ìˆ˜", "IVF ì‹œìˆ  íšŸìˆ˜", 
                     "IVF ì„ì‹  íšŸìˆ˜", "ì´ ì¶œì‚° íšŸìˆ˜", "ì •ì ì¶œì²˜"]

threshold = 0.95
biased_columns = [col for col in X.columns if X[col].value_counts(normalize=True).max() >= threshold]
columns_to_remove.extend(biased_columns)

X.drop(columns=columns_to_remove, inplace=True, errors='ignore')
test.drop(columns=columns_to_remove, inplace=True, errors='ignore')

# âœ… ê²°ì¸¡ì¹˜ ì²˜ë¦¬
missing_percentage = (X.isnull().sum() / len(X)) * 100
high_missing_columns = missing_percentage[missing_percentage >= 80].index.tolist()
X.drop(columns=high_missing_columns, inplace=True, errors='ignore')
test.drop(columns=high_missing_columns, inplace=True, errors='ignore')

mid_missing_columns = missing_percentage[(missing_percentage >= 15) & (missing_percentage < 30)].index.tolist()
imputer = SimpleImputer(strategy='mean')
X[mid_missing_columns] = imputer.fit_transform(X[mid_missing_columns])
test[mid_missing_columns] = imputer.transform(test[mid_missing_columns])

# âœ… ë²”ì£¼í˜• ì»¬ëŸ¼ ì¸ì½”ë”©
categorical_columns = X.select_dtypes(include=['object']).columns.tolist()
test = test[X.columns]

for col in categorical_columns:
    X[col] = X[col].astype(str)
    test[col] = test[col].astype(str)

ordinal_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)
X[categorical_columns] = ordinal_encoder.fit_transform(X[categorical_columns])
test[categorical_columns] = ordinal_encoder.transform(test[categorical_columns])

# âœ… ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ NaN ì±„ìš°ê¸°
numeric_columns = X.select_dtypes(include=['int64', 'float64']).columns.tolist()
X[numeric_columns] = X[numeric_columns].fillna(0)
test[numeric_columns] = test[numeric_columns].fillna(0)

# -------------- ğŸ“Œ Focal Loss ì ìš© --------------
def focal_loss(predt, dtrain, gamma=0.705350567650623, alpha=0.14010485785305138):
    y = dtrain.get_label()
    p = 1 / (1 + np.exp(-predt))
    grad = alpha * (1 - p) ** gamma * (p - y)
    hess = alpha * (1 - p) ** gamma * p * (1 - p) * (1 + gamma * (1 - p) * (y - p))
    return grad, hess

# -------------- ğŸ“Œ XGBoost êµì°¨ ê²€ì¦ --------------
params = {
    "objective": "binary:logistic",
    "eval_metric": "logloss",
    "max_depth": 4,
    "learning_rate":  0.04405038339177713,
    "subsample": 0.8,
    "colsample_bytree": 0.8,
    "random_state": 42
}

n_splits = 5
skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)
auc_scores = []
accuracy_scores = []
cv_results = []

dtrain_full = xgb.DMatrix(X, label=y)

for fold, (train_idx, valid_idx) in enumerate(skf.split(X, y), 1):
    X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]
    y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]
    
    dtrain = xgb.DMatrix(X_train, label=y_train)
    dvalid = xgb.DMatrix(X_valid, label=y_valid)

    model = xgb.train(
        params=params,
        dtrain=dtrain,
        num_boost_round=1000,
        obj=lambda predt, dtrain: focal_loss(predt, dtrain, gamma=2.0, alpha=0.25),
        evals=[(dvalid, "valid")],
        early_stopping_rounds=50,
        verbose_eval=False
    )

    valid_pred_proba = model.predict(dvalid)
    auc = roc_auc_score(y_valid, valid_pred_proba)
    accuracy = accuracy_score(y_valid, (valid_pred_proba > 0.5).astype(int))

    auc_scores.append(auc)
    accuracy_scores.append(accuracy)
    cv_results.append(model.best_iteration)

# âœ… ìµœì ì˜ num_boost_round ì°¾ê¸°
best_num_boost_round = int(np.mean(cv_results))
print(f"ğŸ”¥ ìµœì ì˜ íŠ¸ë¦¬ ê°œìˆ˜: {best_num_boost_round}")

# âœ… ìµœì¢… ëª¨ë¸ í•™ìŠµ (Full Training)
final_model = xgb.train(
    params=params,
    dtrain=dtrain_full,
    num_boost_round=best_num_boost_round,
    verbose_eval=True
)

# âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ ë° ì €ì¥
sample_submission = pd.read_csv('C:/Users/ANTL/Documents/GitHub/LG-Aimers-Data/sample_submission.csv')
dtest = xgb.DMatrix(test)
sample_submission['probability'] = final_model.predict(dtest)
sample_submission.to_csv('C:/Users/ANTL/Documents/GitHub/LG-Aimers-Data/XGBoost_CV_5Fold_FullTrain.csv', index=False)

print("âœ… XGBoost êµì°¨ ê²€ì¦ ì™„ë£Œ & í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ë¨.")
