import numpy as np
import pandas as pd
import optuna
import pickle
from catboost import CatBoostClassifier
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import roc_auc_score

# ë°ì´í„° ë¡œë“œ
file_path_train = "C:/Users/IT/Desktop/LG-Aimers-Data-main/LG-Aimers-Data-main/train.csv"
file_path_test = "C:/Users/IT/Desktop/LG-Aimers-Data-main/LG-Aimers-Data-main/test.csv"
sample_submission_path = "C:/Users/IT/Desktop/LG-Aimers-Data-main/LG-Aimers-Data-main/sample_submission.csv"

df_train = pd.read_csv(file_path_train)
df_test = pd.read_csv(file_path_test)
df_sample_submission = pd.read_csv(sample_submission_path)

# 'ID' ì»¬ëŸ¼ ìœ ì§€ (sample_submissionì„ ìœ„í•´ í•„ìš”)
test_ids = df_sample_submission["ID"]

# íƒ€ê²Ÿ ì»¬ëŸ¼ ë¶„ë¦¬
target_col = "ì„ì‹  ì„±ê³µ ì—¬ë¶€"
X = df_train.drop(columns=["ID", target_col], errors="ignore")
y = df_train[target_col]

# í¸í–¥ëœ ì»¬ëŸ¼ ì œê±°
columns_to_remove = [
    "ì‹ ì„  ë°°ì•„ ì‚¬ìš© ì—¬ë¶€", "ë¯¸ì„¸ì£¼ì…ëœ ë‚œì ìˆ˜", "IVF ì‹œìˆ  íšŸìˆ˜",
    "IVF ì„ì‹  íšŸìˆ˜", "ì´ ì¶œì‚° íšŸìˆ˜", "ì •ì ì¶œì²˜"
]

threshold = 0.95
biased_columns = [col for col in X.columns if X[col].value_counts(normalize=True).max() >= threshold]
columns_to_remove.extend(biased_columns)

X.drop(columns=columns_to_remove, inplace=True, errors="ignore")
df_test.drop(columns=columns_to_remove, inplace=True, errors="ignore")

# ê²°ì¸¡ì¹˜ ì²˜ë¦¬
missing_percentage = (X.isnull().sum() / len(X)) * 100

# 80% ì´ìƒ ê²°ì¸¡ì¹˜ ì»¬ëŸ¼ ì‚­ì œ
high_missing_columns = missing_percentage[missing_percentage >= 80].index.tolist()
X.drop(columns=high_missing_columns, inplace=True, errors="ignore")
df_test.drop(columns=high_missing_columns, inplace=True, errors="ignore")

# 15%~30% ê²°ì¸¡ì¹˜ ì»¬ëŸ¼ í‰ê· ê°’ìœ¼ë¡œ ëŒ€ì²´
mid_missing_columns = missing_percentage[(missing_percentage >= 15) & (missing_percentage < 30)].index.tolist()
X[mid_missing_columns] = X[mid_missing_columns].fillna(X[mid_missing_columns].mean())
df_test[mid_missing_columns] = df_test[mid_missing_columns].fillna(X[mid_missing_columns].mean())

# ë²”ì£¼í˜• ì»¬ëŸ¼ í™•ì¸ (CatBoostì—ì„œ ì§ì ‘ ì²˜ë¦¬ ê°€ëŠ¥)
cat_features = X.select_dtypes(include=["object"]).columns.tolist()

# í…ŒìŠ¤íŠ¸ ë°ì´í„° ì»¬ëŸ¼ ë§ì¶”ê¸°
df_test = df_test.reindex(columns=X.columns, fill_value=0)

# ë²”ì£¼í˜• ë³€ìˆ˜ NaN ê°’ ì²˜ë¦¬ â†’ "missing" ë¬¸ìì—´ë¡œ ë³€í™˜
for col in cat_features:
    X[col] = X[col].fillna("missing").astype(str)
    df_test[col] = df_test[col].fillna("missing").astype(str)

# í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì„¤ì • (ë¶ˆê· í˜• ë°ì´í„° ë³´ì •)
class_weights = {0: 0.25, 1: 0.75}  # ì‹¤íŒ¨(0) -> 0.25, ì„±ê³µ(1) -> 0.75

# Optunaë¥¼ í™œìš©í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” (K-Fold ì ìš©)
def objective(trial):
    params = {
        "iterations": trial.suggest_int("iterations", 500, 3000),
        "depth": trial.suggest_int("depth", 4, 10),
        "learning_rate": trial.suggest_float("learning_rate", 0.005, 0.1, log=True),  # âš ï¸ `suggest_loguniform` â†’ `suggest_float(log=True)`
        "l2_leaf_reg": trial.suggest_float("l2_leaf_reg", 1.0, 50.0, log=True),
        "border_count": trial.suggest_int("border_count", 16, 64),
        "grow_policy": trial.suggest_categorical("grow_policy", ["SymmetricTree", "Lossguide", "Depthwise"]),
        "class_weights": [class_weights[0], class_weights[1]],  # ê°€ì¤‘ì¹˜ ì ìš©
        "random_seed": 42,
        "eval_metric": "AUC",
        "loss_function": "Logloss",
        "task_type": "GPU",   # âœ… GPU ì‚¬ìš© ì„¤ì •
        "devices": "0",       # âœ… íŠ¹ì • GPU (GPU 0ë²ˆ) ì‚¬ìš©
        "verbose": 0
    }
    
    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)  # 5-Fold êµì°¨ ê²€ì¦
    auc_scores = []
    
    for train_idx, valid_idx in kf.split(X, y):
        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]
        y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]

        model = CatBoostClassifier(**params)
        model.fit(
            X_train, y_train,
            eval_set=(X_valid, y_valid),
            cat_features=cat_features,
            early_stopping_rounds=100,
            verbose=0
        )
        
        valid_preds = model.predict_proba(X_valid)[:, 1]
        auc_scores.append(roc_auc_score(y_valid, valid_preds))
    
    return np.mean(auc_scores)  # K-Fold í‰ê·  AUC ë°˜í™˜

# Optuna ì‹¤í–‰ (ìµœì í™”)
study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=30)

# ìµœì  íŒŒë¼ë¯¸í„° ì €ì¥ (`pkl` íŒŒì¼)
best_params = study.best_params
best_params["random_seed"] = 42
best_params["eval_metric"] = "AUC"
best_params["loss_function"] = "Logloss"
best_params["task_type"] = "GPU"   # âœ… GPU ì‚¬ìš©
best_params["devices"] = "0"       # âœ… íŠ¹ì • GPU ì‚¬ìš©
best_params["verbose"] = 100

# GPU ì‚¬ìš© ì‹œ ë¶ˆí•„ìš”í•œ íŒŒë¼ë¯¸í„° ì œê±°
if "colsample_bylevel" in best_params:
    del best_params["colsample_bylevel"]

# ìµœì í™”ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ pkl íŒŒì¼ë¡œ ì €ì¥
params_save_path = "best_catboost_params_kfold.pkl"
with open(params_save_path, "wb") as f:
    pickle.dump(best_params, f)

print(f"ğŸ“ ìµœì í™”ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„°ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {params_save_path}")
print(f"ğŸ¯ ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°: {best_params}")

# ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì ìš©í•˜ì—¬ ì „ì²´ ë°ì´í„° í•™ìŠµ (K-Fold X, ìµœì¢… ëª¨ë¸ ìƒì„±)
best_params["class_weights"] = [class_weights[0], class_weights[1]]

final_model = CatBoostClassifier(**best_params)
final_model.fit(
    X, y,  # ì „ì²´ ë°ì´í„° ì‚¬ìš©
    cat_features=cat_features,
    verbose=100
)

# í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ (í™•ë¥ ê°’ ì €ì¥)
X_test = df_test
test_preds = final_model.predict_proba(X_test)[:, 1]  # í™•ë¥ ê°’ ì €ì¥

# sample_submission í˜•ì‹ìœ¼ë¡œ ë³€í™˜
df_submission = pd.DataFrame({"ID": test_ids, "probability": test_preds})

# ìµœì¢… CSV íŒŒì¼ ì €ì¥
submission_file_path = "C:/Users/IT/Desktop/LG-Aimers-Data-main/LG-Aimers-Data-main/catboost_kfold_weight.csv"
df_submission.to_csv(submission_file_path, index=False)

print(f"âœ… ìµœì í™”ëœ CatBoost ëª¨ë¸ì˜ ì˜ˆì¸¡ ê²°ê³¼ê°€ '{submission_file_path}' ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")