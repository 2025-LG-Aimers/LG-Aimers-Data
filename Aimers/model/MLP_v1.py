import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler
from sklearn.impute import SimpleImputer
from sklearn.metrics import roc_auc_score, accuracy_score

# -------------- ğŸ“Œ ë°ì´í„° ë¡œë”© --------------
train = pd.read_csv('C:/Users/ANTL/Documents/GitHub/LG-Aimers-Data/train.csv').drop(columns=['ID'])
test = pd.read_csv('C:/Users/ANTL/Documents/GitHub/LG-Aimers-Data/test.csv').drop(columns=['ID'])

# -------------- ğŸ“Œ íƒ€ê²Ÿ ë³€ìˆ˜ ë¶„ë¦¬ --------------
X = train.drop(columns=['ì„ì‹  ì„±ê³µ ì—¬ë¶€'])  # ì…ë ¥ ë°ì´í„° (Feature)
y = train['ì„ì‹  ì„±ê³µ ì—¬ë¶€']  # íƒ€ê²Ÿ ë³€ìˆ˜ (Label)

# -------------- ğŸ“Œ Train-Test Split --------------
X_train, X_valid, y_train, y_valid = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"ğŸ”¹ í•™ìŠµ ë°ì´í„° í¬ê¸°: {X_train.shape}, ê²€ì¦ ë°ì´í„° í¬ê¸°: {X_valid.shape}")

# -------------- ğŸ“Œ ê²°ì¸¡ì¹˜ ì²˜ë¦¬ --------------
missing_percentage = (X_train.isnull().sum() / len(X_train)) * 100

# âœ… 1. ê²°ì¸¡ì¹˜ ë¹„ìœ¨ì´ 80% ì´ìƒì¸ ì»¬ëŸ¼ ì‚­ì œ
high_missing_columns = missing_percentage[missing_percentage >= 80].index
X_train.drop(columns=high_missing_columns, inplace=True, errors='ignore')
X_valid.drop(columns=high_missing_columns, inplace=True, errors='ignore')
test.drop(columns=high_missing_columns, inplace=True, errors='ignore')

# âœ… 2. ê²°ì¸¡ì¹˜ ë¹„ìœ¨ì´ 15% ~ 30%ì¸ ì»¬ëŸ¼ í‰ê· ê°’ìœ¼ë¡œ ëŒ€ì²´
mid_missing_columns = missing_percentage[(missing_percentage >= 15) & (missing_percentage < 30)].index
imputer = SimpleImputer(strategy='mean')  # í‰ê· ê°’ ëŒ€ì²´
X_train[mid_missing_columns] = imputer.fit_transform(X_train[mid_missing_columns])
X_valid[mid_missing_columns] = imputer.transform(X_valid[mid_missing_columns])
test[mid_missing_columns] = imputer.transform(test[mid_missing_columns])

# âœ… 3. ë²”ì£¼í˜• ì»¬ëŸ¼ ìë™ ê°ì§€ & ì¸ì½”ë”© (Ordinal Encoding)
categorical_columns = X_train.select_dtypes(include=['object']).columns.tolist()
print(f"ğŸ“Œ ê°ì§€ëœ ë²”ì£¼í˜• ì»¬ëŸ¼: {categorical_columns}")

# ğŸ”¥ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œë„ ë™ì¼í•œ ì»¬ëŸ¼ ìœ ì§€
test = test[X_train.columns]

# ë²”ì£¼í˜• ë°ì´í„°ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
for col in categorical_columns:
    X_train[col] = X_train[col].astype(str)
    X_valid[col] = X_valid[col].astype(str)
    test[col] = test[col].astype(str)

# âœ… OrdinalEncoder ì„¤ì • ë° ë³€í™˜
ordinal_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)
X_train[categorical_columns] = ordinal_encoder.fit_transform(X_train[categorical_columns])
X_valid[categorical_columns] = ordinal_encoder.transform(X_valid[categorical_columns])
test[categorical_columns] = ordinal_encoder.transform(test[categorical_columns])

# -------------- ğŸ“Œ ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ìë™ ê°ì§€ & ì •ê·œí™” --------------
numeric_columns = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()
print(f"ğŸ“Œ ê°ì§€ëœ ìˆ˜ì¹˜í˜• ì»¬ëŸ¼: {numeric_columns}")

# âœ… MinMaxScaler ì ìš© (ì‹ ê²½ë§ í•™ìŠµ ì•ˆì •ì„± í–¥ìƒ)
scaler = MinMaxScaler()
X_train[numeric_columns] = scaler.fit_transform(X_train[numeric_columns])
X_valid[numeric_columns] = scaler.transform(X_valid[numeric_columns])
test[numeric_columns] = scaler.transform(test[numeric_columns])

# -------------- ğŸ“Œ MLP ì‹ ê²½ë§ ëª¨ë¸ êµ¬ì¶• --------------
model = Sequential([
    Dense(256, activation='relu', input_shape=(X_train.shape[1],)),  # ì…ë ¥ì¸µ
    Dropout(0.3),  # ê³¼ì í•© ë°©ì§€
    Dense(128, activation='relu'),
    Dropout(0.3),
    Dense(64, activation='relu'),
    Dropout(0.2),
    Dense(1, activation='sigmoid')  # ì¶œë ¥ì¸µ (ì´ì§„ ë¶„ë¥˜ â†’ sigmoid)
])

# -------------- ğŸ“Œ ëª¨ë¸ ì»´íŒŒì¼ --------------
model.compile(
    optimizer=Adam(learning_rate=0.001),  # Adam Optimizer
    loss='binary_crossentropy',  # ì´ì§„ ë¶„ë¥˜ì´ë¯€ë¡œ Binary Crossentropy ì‚¬ìš©
    metrics=['accuracy']
)

# -------------- ğŸ“Œ Early Stopping ì„¤ì • --------------
early_stopping = EarlyStopping(
    monitor='val_loss', patience=10, restore_best_weights=True
)

# -------------- ğŸ“Œ ëª¨ë¸ í•™ìŠµ --------------
history = model.fit(
    X_train, y_train,
    validation_data=(X_valid, y_valid),
    epochs=100,  # ìµœëŒ€ 100 Epochê¹Œì§€ í•™ìŠµ
    batch_size=512,  # ë°°ì¹˜ í¬ê¸° (ë„ˆë¬´ ì‘ìœ¼ë©´ ëŠë¦¬ê³ , ë„ˆë¬´ í¬ë©´ ë©”ëª¨ë¦¬ ë¶€ì¡±)
    callbacks=[early_stopping],  # Early Stopping ì ìš©
    verbose=1
)

# -------------- ğŸ“Œ ê²€ì¦ ë°ì´í„°ì—ì„œ ROC-AUC ë° Accuracy í‰ê°€ --------------
valid_pred_proba = model.predict(X_valid).flatten()
valid_pred_class = (valid_pred_proba > 0.5).astype(int)

auc_score = roc_auc_score(y_valid, valid_pred_proba)
accuracy = accuracy_score(y_valid, valid_pred_class)

print(f"ğŸ”¥ ê²€ì¦ ë°ì´í„° ROC-AUC Score: {auc_score:.4f}")
print(f"âœ… ê²€ì¦ ë°ì´í„° Accuracy Score: {accuracy:.4f}")

# -------------- ğŸ“Œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ ë° ê²°ê³¼ ì €ì¥ --------------
test_pred_proba = model.predict(test).flatten()

sample_submission = pd.read_csv('C:/Users/ANTL/Documents/GitHub/LG-Aimers-Data/sample_submission.csv')
sample_submission['probability'] = test_pred_proba
sample_submission.to_csv('C:/Users/ANTL/Documents/GitHub/LG-Aimers-Data/baseline_submit_mlp.csv', index=False)

print("âœ… MLP ëª¨ë¸ í•™ìŠµ & ì˜ˆì¸¡ ì™„ë£Œ, ê²°ê³¼ ì €ì¥ë¨.")
