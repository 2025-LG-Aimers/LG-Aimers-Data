import pandas as pd
import numpy as np
import xgboost as xgb  # âœ… XGBoost ì‚¬ìš©
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OrdinalEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.svm import LinearSVC  # âœ… ì„ í˜• SVC ì¶”ê°€
from sklearn.metrics import roc_auc_score, accuracy_score

# -------------- ğŸ“Œ ë°ì´í„° ë¡œë”© --------------
train = pd.read_csv('C:/Users/ANTL/Documents/GitHub/LG-Aimers-Data/train.csv').drop(columns=['ID'])
test = pd.read_csv('C:/Users/ANTL/Documents/GitHub/LG-Aimers-Data/test.csv').drop(columns=['ID'])

# -------------- ğŸ“Œ íƒ€ê²Ÿ ë³€ìˆ˜ ë¶„ë¦¬ --------------
X = train.drop(columns=['ì„ì‹  ì„±ê³µ ì—¬ë¶€'])  # ì…ë ¥ ë°ì´í„° (Feature)
y = train['ì„ì‹  ì„±ê³µ ì—¬ë¶€']  # íƒ€ê²Ÿ ë³€ìˆ˜ (Label)

# âœ… 1. í›ˆë ¨(66%)ê³¼ ê²€ì¦(34%)ìœ¼ë¡œ ë°ì´í„° ë¶„í• 
X_train, X_valid, y_train, y_valid = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"ğŸ”¹ í•™ìŠµ ë°ì´í„° í¬ê¸°: {X_train.shape}, ê²€ì¦ ë°ì´í„° í¬ê¸°: {X_valid.shape}")

# âœ… 2. ëª…ëª©í˜•(Nominal) vs. ìˆœì„œí˜•(Ordinal) ì»¬ëŸ¼ êµ¬ë¶„
ordinal_columns = [
    'ì‹œìˆ  ë‹¹ì‹œ ë‚˜ì´', 'ì´ ì‹œìˆ  íšŸìˆ˜', 'í´ë¦¬ë‹‰ ë‚´ ì´ ì‹œìˆ  íšŸìˆ˜', 'IVF ì‹œìˆ  íšŸìˆ˜',
    'DI ì‹œìˆ  íšŸìˆ˜', 'ì´ ì„ì‹  íšŸìˆ˜', 'IVF ì„ì‹  íšŸìˆ˜', 'DI ì„ì‹  íšŸìˆ˜',
    'ì´ ì¶œì‚° íšŸìˆ˜', 'IVF ì¶œì‚° íšŸìˆ˜'
]

nominal_columns = [
    'ì‹œìˆ  ì‹œê¸° ì½”ë“œ', 'ì‹œìˆ  ìœ í˜•', 'íŠ¹ì • ì‹œìˆ  ìœ í˜•', 'ë°°ë€ ìœ ë„ ìœ í˜•',
    'ë°°ì•„ ìƒì„± ì£¼ìš” ì´ìœ ', 'ë‚œì ì¶œì²˜', 'ì •ì ì¶œì²˜',
    'ë‚œì ê¸°ì¦ì ë‚˜ì´', 'ì •ì ê¸°ì¦ì ë‚˜ì´'
]

# âœ… 3. í¸í–¥ëœ ì»¬ëŸ¼ íƒìƒ‰ & ì œê±°
threshold = 0.99
biased_columns = [col for col in X_train.columns if X_train[col].value_counts(normalize=True).max() >= threshold]
X_train.drop(columns=biased_columns, inplace=True, errors='ignore')
X_valid.drop(columns=biased_columns, inplace=True, errors='ignore')
test.drop(columns=biased_columns, inplace=True, errors='ignore')

print(f"âœ… ì œê±°ëœ ì»¬ëŸ¼: {biased_columns}")

# âœ… 4. ê²°ì¸¡ì¹˜ ì²˜ë¦¬ (ìˆ˜ì¹˜í˜• ë°ì´í„° & ë²”ì£¼í˜• ë°ì´í„° ë¶„ë¦¬)
numeric_columns = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()

# âœ… 4-1. ìˆ˜ì¹˜í˜• ë°ì´í„° ê²°ì¸¡ì¹˜ í‰ê· ê°’ ëŒ€ì²´
num_imputer = SimpleImputer(strategy='mean')
X_train[numeric_columns] = num_imputer.fit_transform(X_train[numeric_columns])
X_valid[numeric_columns] = num_imputer.transform(X_valid[numeric_columns])
test[numeric_columns] = num_imputer.transform(test[numeric_columns])

# âœ… 4-2. ëª…ëª©í˜• ë°ì´í„° ê²°ì¸¡ì¹˜ëŠ” "missing"ìœ¼ë¡œ ì±„ìš°ê¸°
X_train[nominal_columns] = X_train[nominal_columns].fillna("missing")
X_valid[nominal_columns] = X_valid[nominal_columns].fillna("missing")
test[nominal_columns] = test[nominal_columns].fillna("missing")

# âœ… 5. ìˆœì„œí˜• ë°ì´í„°(Ordinal) â†’ Ordinal Encoding ì ìš©
ordinal_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)
X_train[ordinal_columns] = ordinal_encoder.fit_transform(X_train[ordinal_columns])
X_valid[ordinal_columns] = ordinal_encoder.transform(X_valid[ordinal_columns])
test[ordinal_columns] = ordinal_encoder.transform(test[ordinal_columns])

# âœ… 6. ëª…ëª©í˜• ë°ì´í„°(Nominal) â†’ Target Encoding ì ìš©
for col in nominal_columns:
    target_mean = train.groupby(col)['ì„ì‹  ì„±ê³µ ì—¬ë¶€'].mean()
    X_train[col] = X_train[col].map(target_mean)
    X_valid[col] = X_valid[col].map(target_mean).fillna(X_train[col].mean())
    test[col] = test[col].map(target_mean).fillna(X_train[col].mean())

# -------------- ğŸ“Œ XGBoost ëª¨ë¸ í•™ìŠµ --------------
params = {
    "objective": "binary:logistic",
    "eval_metric": "logloss",
    "max_depth": 6,
    "learning_rate": 0.03,
    "subsample": 0.8,
    "colsample_bytree": 0.8,
    "random_state": 42
}

# XGBoost ì „ìš© DMatrix ìƒì„±
dtrain = xgb.DMatrix(X_train, label=y_train)

# ğŸ”¥ XGBoost ëª¨ë¸ í•™ìŠµ
xgb_model = xgb.train(
    params=params,
    dtrain=dtrain,
    num_boost_round=500,
    verbose_eval=True
)

# âœ… XGBoostì—ì„œ ì¤‘ìš”í•œ íŠ¹ì§• ì„ íƒ (ìƒìœ„ 10ê°œ)
feature_importance = xgb_model.get_score(importance_type="weight")
sorted_features = sorted(feature_importance, key=feature_importance.get, reverse=True)[:10]
X_train_selected = X_train[sorted_features]
X_valid_selected = X_valid[sorted_features]
test_selected = test[sorted_features]

print(f"âœ… ì„ íƒëœ íŠ¹ì§•: {sorted_features}")

# âœ… 7. ë°ì´í„° ì •ê·œí™” (Linear SVCëŠ” ì •ê·œí™”ê°€ í•„ìš”í•¨)
scaler = StandardScaler()
X_train_selected = scaler.fit_transform(X_train_selected)
X_valid_selected = scaler.transform(X_valid_selected)
test_selected = scaler.transform(test_selected)

# âœ… 8. Linear SVC ëª¨ë¸ í•™ìŠµ
svc_model = LinearSVC(max_iter=10000, random_state=42)
svc_model.fit(X_train_selected, y_train)

# -------------- ğŸ“Œ ê²€ì¦ ë°ì´í„°ì—ì„œ ROC-AUC ë° Accuracy í‰ê°€ --------------
valid_pred_class = svc_model.predict(X_valid_selected)
auc_score = roc_auc_score(y_valid, valid_pred_class)
accuracy = accuracy_score(y_valid, valid_pred_class)

print(f"ğŸ”¥ Linear SVC ê²€ì¦ ë°ì´í„° ROC-AUC Score: {auc_score:.4f}")
print(f"âœ… Linear SVC ê²€ì¦ ë°ì´í„° Accuracy Score: {accuracy:.4f}")

# -------------- ğŸ“Œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ ë° ê²°ê³¼ ì €ì¥ --------------
test_pred_class = svc_model.predict(test_selected)

sample_submission = pd.read_csv('C:/Users/ANTL/Documents/GitHub/LG-Aimers-Data/sample_submission.csv')
sample_submission['probability'] = test_pred_class
sample_submission.to_csv('C:/Users/ANTL/Documents/GitHub/LG-Aimers-Data/XGBoost_SVC.csv', index=False)

print("âœ… XGBoost íŠ¹ì§• ì„ íƒ + Linear SVC í•™ìŠµ ì™„ë£Œ, ê²°ê³¼ ì €ì¥ë¨.")
