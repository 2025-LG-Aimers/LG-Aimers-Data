import pandas as pd
import numpy as np
import xgboost as xgb  # âœ… XGBoost ì „ì²´ ëª¨ë“ˆ ì‚¬ìš©
from sklearn.preprocessing import OrdinalEncoder, PolynomialFeatures
from sklearn.impute import SimpleImputer
from sklearn.metrics import roc_auc_score, accuracy_score

# -------------- ğŸ“Œ ë°ì´í„° ë¡œë”© --------------
train = pd.read_csv('C:/Users/ANTL/Documents/GitHub/LG-Aimers-Data/train.csv').drop(columns=['ID'])
test = pd.read_csv('C:/Users/ANTL/Documents/GitHub/LG-Aimers-Data/test.csv').drop(columns=['ID'])

# -------------- ğŸ“Œ íƒ€ê²Ÿ ë³€ìˆ˜ ë¶„ë¦¬ --------------
X = train.drop(columns=['ì„ì‹  ì„±ê³µ ì—¬ë¶€'])
y = train['ì„ì‹  ì„±ê³µ ì—¬ë¶€']

# âœ… ì œê±°í•  ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
columns_to_remove = [
    "ì‹ ì„  ë°°ì•„ ì‚¬ìš© ì—¬ë¶€", "ë¯¸ì„¸ì£¼ì…ëœ ë‚œì ìˆ˜", "IVF ì‹œìˆ  íšŸìˆ˜",
    "IVF ì„ì‹  íšŸìˆ˜", "ì´ ì¶œì‚° íšŸìˆ˜", "ì •ì ì¶œì²˜", "ë°°ë€ ìê·¹ ì—¬ë¶€"
]

# âœ… í¸í–¥ëœ ì»¬ëŸ¼ íƒìƒ‰ (95% ì´ìƒ í•œ ê°’ ì œê±°)
threshold = 0.95
biased_columns = [col for col in X.columns if X[col].value_counts(normalize=True).max() >= threshold]
columns_to_remove.extend(biased_columns)

# âœ… ì»¬ëŸ¼ ì‚­ì œ ì ìš©
X.drop(columns=columns_to_remove, inplace=True, errors='ignore')
test.drop(columns=columns_to_remove, inplace=True, errors='ignore')

print(f"âœ… ì œê±°ëœ ì»¬ëŸ¼: {columns_to_remove}")

# -------------- ğŸ“Œ ê²°ì¸¡ì¹˜ ì²˜ë¦¬ --------------
# âœ… 1. ê²°ì¸¡ì¹˜ ë¹„ìœ¨ì´ 80% ì´ìƒì¸ ì»¬ëŸ¼ ì‚­ì œ
missing_percentage = (X.isnull().sum() / len(X)) * 100
high_missing_columns = missing_percentage[missing_percentage >= 80].index.tolist()

X.drop(columns=high_missing_columns, inplace=True, errors='ignore')
test.drop(columns=high_missing_columns, inplace=True, errors='ignore')

# âœ… 2. ë²”ì£¼í˜• ì»¬ëŸ¼ ì¸ì½”ë”© (Ordinal Encoding)
categorical_columns = X.select_dtypes(include=['object']).columns.tolist()
print(f"ğŸ“Œ ê°ì§€ëœ ë²”ì£¼í˜• ì»¬ëŸ¼: {categorical_columns}")

test = test[X.columns]  # ğŸ”¥ ë™ì¼í•œ ì»¬ëŸ¼ ìœ ì§€

ordinal_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)
X[categorical_columns] = ordinal_encoder.fit_transform(X[categorical_columns])
test[categorical_columns] = ordinal_encoder.transform(test[categorical_columns])

# âœ… 3. ìˆ˜ì¹˜í˜• ë°ì´í„°ë§Œ ê²°ì¸¡ì¹˜ í‰ê·  ëŒ€ì²´
numeric_columns = X.select_dtypes(include=['int64', 'float64']).columns.tolist()
imputer = SimpleImputer(strategy='mean')
X[numeric_columns] = imputer.fit_transform(X[numeric_columns])
test[numeric_columns] = imputer.transform(test[numeric_columns])

# âœ… NaN ê°œìˆ˜ í™•ì¸ (í™•ì¸ìš©)
print(f"ğŸ” NaN ê°œìˆ˜ í™•ì¸: {X.isnull().sum().sum()}")

# -------------- ğŸ“Œ ë‹¤í•­ Feature ìƒì„± --------------
print(f"ğŸ“Œ ê°ì§€ëœ ìˆ˜ì¹˜í˜• ì»¬ëŸ¼: {numeric_columns}")

# âœ… Polynomial Features ì ìš© (2ì°¨ ìƒí˜¸ì‘ìš© í•­ë§Œ ì¶”ê°€)
poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)
X_poly = poly.fit_transform(X[numeric_columns])
test_poly = poly.transform(test[numeric_columns])

# âœ… ë‹¤í•­ Feature DataFrame ë³€í™˜ ë° ê¸°ì¡´ ë°ì´í„°ì™€ ë³‘í•©
poly_feature_names = poly.get_feature_names_out(numeric_columns)
X_poly_df = pd.DataFrame(X_poly, columns=poly_feature_names, index=X.index)
test_poly_df = pd.DataFrame(test_poly, columns=poly_feature_names, index=test.index)

X = pd.concat([X, X_poly_df], axis=1)
test = pd.concat([test, test_poly_df], axis=1)

print(f"âœ… ì¶”ê°€ëœ ë‹¤í•­ Features: {len(poly_feature_names)} ê°œ")
print(f"ğŸ”¹ ë‹¤í•­ Feature ì˜ˆì‹œ: {poly_feature_names[:5]}")

# âœ… ë°ì´í„° íƒ€ì… ë³€í™˜ (float32ë¡œ ë³€í™˜í•˜ì—¬ ì˜¤ë¥˜ ë°©ì§€)
X = X.astype(np.float32)
test = test.astype(np.float32)

# âœ… Feature ê°œìˆ˜ í™•ì¸
print(f"ğŸ” Feature ê°œìˆ˜ (X): {X.shape}, (test): {test.shape}")

# âœ… X_train, y_train (Full Train) - ë°ì´í„° ì „ì²´ í•™ìŠµ
X_train = X.to_numpy()
y_train = y.to_numpy()

# âœ… XGBoost ì „ìš© DMatrix ìƒì„±
try:
    dtrain = xgb.DMatrix(X_train, label=y_train)
except Exception as e:
    print(f"ğŸš¨ DMatrix ë³€í™˜ ì˜¤ë¥˜ ë°œìƒ: {e}")
    exit()

# -------------- ğŸ“Œ XGBoost ëª¨ë¸ í•™ìŠµ (Full Train) --------------
params = {
    "objective": "binary:logistic",
    "eval_metric": "logloss",
    "max_depth": 6,
    "learning_rate": 0.03,
    "subsample": 0.8,
    "colsample_bytree": 0.8,
    "random_state": 42
}

# ğŸ”¥ Full Train â†’ ê²€ì¦ ë°ì´í„° ì—†ìŒ (Early Stopping ì œê±°)
xgb_model = xgb.train(
    params=params,
    dtrain=dtrain,
    num_boost_round=1000,  # âœ… 1000ë²ˆ í•™ìŠµ
    verbose_eval=True
)

# âœ… í•™ìŠµ ë°ì´í„° ì˜ˆì¸¡ ë° í‰ê°€
train_pred_proba = xgb_model.predict(dtrain)
train_pred_class = (train_pred_proba > 0.5).astype(int)

train_auc_score = roc_auc_score(y_train, train_pred_proba)
train_accuracy = accuracy_score(y_train, train_pred_class)

print(f"ğŸ”¥ í•™ìŠµ ë°ì´í„° ROC-AUC Score: {train_auc_score:.10f}")
print(f"âœ… í•™ìŠµ ë°ì´í„° Accuracy Score: {train_accuracy:.10f}")

# âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ ë° ê²°ê³¼ ì €ì¥
test = test.to_numpy()  # âœ… XGBoost DMatrix ë³€í™˜ ì „ NumPy ë°°ì—´ë¡œ ë³€í™˜
dtest = xgb.DMatrix(test)
test_pred_proba = xgb_model.predict(dtest)

sample_submission = pd.read_csv('C:/Users/ANTL/Documents/GitHub/LG-Aimers-Data/sample_submission.csv')
sample_submission['probability'] = test_pred_proba
sample_submission.to_csv('C:/Users/ANTL/Documents/GitHub/LG-Aimers-Data/XGBoost_FullTrain_PolyFeatures.csv', index=False)

print("âœ… Full Train ì™„ë£Œ! ë‹¤í•­ì‹ Feature ì ìš© í›„ XGBoost ëª¨ë¸ í•™ìŠµ & ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ë¨.")
