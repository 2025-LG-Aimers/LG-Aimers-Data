import pandas as pd
import numpy as np
import xgboost as xgb  # âœ… XGBoost ì‚¬ìš©
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OrdinalEncoder
from sklearn.impute import SimpleImputer
from sklearn.metrics import roc_auc_score, accuracy_score


# âœ… í•œê¸€ í°íŠ¸ ê²½ë¡œ ì„¤ì •
plt.rc('font', family='Malgun Gothic')  # Windows: 'Malgun Gothic', Mac: 'AppleGothic'

# âœ… ìŒìˆ˜ ê¸°í˜¸(ë§ˆì´ë„ˆìŠ¤) ê¹¨ì§ ë°©ì§€
plt.rcParams['axes.unicode_minus'] = False  

# -------------- ğŸ“Œ ë°ì´í„° ë¡œë”© --------------
train = pd.read_csv('C:/Users/ANTL/Documents/GitHub/LG-Aimers-Data/train.csv').drop(columns=['ID'])
test = pd.read_csv('C:/Users/ANTL/Documents/GitHub/LG-Aimers-Data/test.csv').drop(columns=['ID'])

# -------------- ğŸ“Œ íƒ€ê²Ÿ ë³€ìˆ˜ ë¶„ë¦¬ --------------
X = train.drop(columns=['ì„ì‹  ì„±ê³µ ì—¬ë¶€'])  # Feature ë°ì´í„°
y = train['ì„ì‹  ì„±ê³µ ì—¬ë¶€']  # Target ë°ì´í„°

# âœ… í¸í–¥ëœ ì»¬ëŸ¼ íƒìƒ‰ & ì œê±° (95% ì´ìƒ ê°™ì€ ê°’)
threshold = 0.95
biased_columns = [col for col in X.columns if X[col].value_counts(normalize=True).max() >= threshold]

X.drop(columns=biased_columns, inplace=True, errors='ignore')
test.drop(columns=biased_columns, inplace=True, errors='ignore')

print(f"âœ… ì œê±°ëœ ì»¬ëŸ¼: {biased_columns}")

# -------------- ğŸ“Œ Train-Test Split --------------
X_train, X_valid, y_train, y_valid = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"ğŸ”¹ í•™ìŠµ ë°ì´í„° í¬ê¸°: {X_train.shape}, ê²€ì¦ ë°ì´í„° í¬ê¸°: {X_valid.shape}")

# -------------- ğŸ“Œ ê²°ì¸¡ì¹˜ ì²˜ë¦¬ --------------
missing_percentage = (X_train.isnull().sum() / len(X_train)) * 100

# âœ… 1. ê²°ì¸¡ì¹˜ ë¹„ìœ¨ì´ 80% ì´ìƒì¸ ì»¬ëŸ¼ ì‚­ì œ
high_missing_columns = missing_percentage[missing_percentage >= 80].index
X_train.drop(columns=high_missing_columns, inplace=True, errors='ignore')
X_valid.drop(columns=high_missing_columns, inplace=True, errors='ignore')
test.drop(columns=high_missing_columns, inplace=True, errors='ignore')

# âœ… 2. ê²°ì¸¡ì¹˜ ë¹„ìœ¨ì´ 15% ~ 30%ì¸ ì»¬ëŸ¼ í‰ê· ê°’ìœ¼ë¡œ ëŒ€ì²´
mid_missing_columns = missing_percentage[(missing_percentage >= 15) & (missing_percentage < 30)].index
imputer = SimpleImputer(strategy='mean')
X_train[mid_missing_columns] = imputer.fit_transform(X_train[mid_missing_columns])
X_valid[mid_missing_columns] = imputer.transform(X_valid[mid_missing_columns])
test[mid_missing_columns] = imputer.transform(test[mid_missing_columns])

# âœ… 3. ë²”ì£¼í˜• ì»¬ëŸ¼ ìë™ ê°ì§€ & ì¸ì½”ë”© (Ordinal Encoding)
categorical_columns = X_train.select_dtypes(include=['object']).columns.tolist()
print(f"ğŸ“Œ ê°ì§€ëœ ë²”ì£¼í˜• ì»¬ëŸ¼: {categorical_columns}")

test = test[X_train.columns]  # ğŸ”¥ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì»¬ëŸ¼ ìœ ì§€

# âœ… Ordinal Encoding ì ìš©
ordinal_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)
X_train[categorical_columns] = ordinal_encoder.fit_transform(X_train[categorical_columns])
X_valid[categorical_columns] = ordinal_encoder.transform(X_valid[categorical_columns])
test[categorical_columns] = ordinal_encoder.transform(test[categorical_columns])

# -------------- ğŸ“Œ XGBoost ëª¨ë¸ í•™ìŠµ --------------
params = {
    "objective": "binary:logistic",
    "eval_metric": ["logloss"],
    "max_depth": 5,  # 6 -> 5 (ì„±ëŠ¥ ê°œì„ )
    "learning_rate": 0.03,
    "subsample": 0.8,
    "colsample_bytree": 0.83,
    "random_state": 42
}

# âœ… DMatrix ìƒì„±
dtrain = xgb.DMatrix(X_train, label=y_train)
dvalid = xgb.DMatrix(X_valid, label=y_valid)

# ğŸ”¥ Early Stopping ì ìš©
watchlist = [(dtrain, "train"), (dvalid, "valid")]
xgb_model = xgb.train(
    params=params,
    dtrain=dtrain,
    num_boost_round=1000,
    evals=watchlist,
    early_stopping_rounds=50,
    verbose_eval=True
)

# âœ… Feature Importance ê°€ì ¸ì˜¤ê¸° (Gain ê¸°ì¤€)
importances_gain = xgb_model.get_score(importance_type='gain')

# DataFrame ë³€í™˜
importance_df = pd.DataFrame({
    'Feature': list(importances_gain.keys()),
    'Gain': list(importances_gain.values())  # ì •ë³´ëŸ‰ ê¸°ì—¬ë„
})

# ğŸ”¥ Feature Importance (Gain) ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬
importance_df = importance_df.sort_values(by='Gain', ascending=False)

# âœ… ìƒìœ„ 20ê°œ Feature ì‹œê°í™”
plt.figure(figsize=(12, 6))
plt.barh(importance_df['Feature'][:20], importance_df['Gain'][:20], color='skyblue')
plt.xlabel("Feature Importance (Gain)")
plt.ylabel("Features")
plt.title("Top 20 Most Important Features (by Gain)")
plt.gca().invert_yaxis()  # ê°€ì¥ ì¤‘ìš”í•œ Featureê°€ ìœ„ë¡œ ì˜¤ë„ë¡ ë°˜ì „
plt.show()

# âœ… í•˜ìœ„ 25% Feature ì œê±°
threshold = importance_df['Gain'].quantile(0.25)
low_gain_features = importance_df[importance_df['Gain'] < threshold]['Feature'].tolist()

# ğŸ”¥ Feature ì œê±° í›„ ë‹¤ì‹œ í•™ìŠµ
X_train.drop(columns=low_gain_features, inplace=True)
X_valid.drop(columns=low_gain_features, inplace=True)
test.drop(columns=low_gain_features, inplace=True)

print(f"ğŸ“Œ ì œê±°ëœ í”¼ì²˜ (í•˜ìœ„ 25%): {low_gain_features}")

# âœ… DMatrix ë‹¤ì‹œ ìƒì„± í›„ ì¬í•™ìŠµ
dtrain = xgb.DMatrix(X_train, label=y_train)
dvalid = xgb.DMatrix(X_valid, label=y_valid)

xgb_model = xgb.train(
    params=params,
    dtrain=dtrain,
    num_boost_round=1000,
    evals=[(dtrain, "train"), (dvalid, "valid")],
    early_stopping_rounds=50,
    verbose_eval=True
)

# -------------- ğŸ“Œ ê²€ì¦ ë°ì´í„°ì—ì„œ ROC-AUC ë° Accuracy í‰ê°€ --------------
valid_pred_proba = xgb_model.predict(dvalid)
valid_pred_class = (valid_pred_proba > 0.5).astype(int)

auc_score = roc_auc_score(y_valid, valid_pred_proba)
accuracy = accuracy_score(y_valid, valid_pred_class)
print(f"ğŸ”¥ ê²€ì¦ ë°ì´í„° ROC-AUC Score: {auc_score:.4f}")
print(f"âœ… ê²€ì¦ ë°ì´í„° Accuracy Score: {accuracy:.4f}")

# -------------- ğŸ“Œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ ë° ê²°ê³¼ ì €ì¥ --------------
dtest = xgb.DMatrix(test)
test_pred_proba = xgb_model.predict(dtest)

sample_submission = pd.read_csv('C:/Users/ANTL/Documents/GitHub/LG-Aimers-Data/sample_submission.csv')
sample_submission['probability'] = test_pred_proba
sample_submission.to_csv('C:/Users/ANTL/Documents/GitHub/LG-Aimers-Data/XGBoost_gain(25%).csv', index=False)

print("âœ… XGBoost ëª¨ë¸ í•™ìŠµ & ì˜ˆì¸¡ ì™„ë£Œ, ê²°ê³¼ ì €ì¥ë¨.")
