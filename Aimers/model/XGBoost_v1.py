import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OrdinalEncoder
from xgboost import XGBClassifier  # âœ… XGBoost ì¶”ê°€
from sklearn.metrics import roc_auc_score, accuracy_score

# -------------- ğŸ“Œ ë°ì´í„° ë¡œë”© -------------- 
train = pd.read_csv('C:/Users/ANTL/Desktop/LG Aimers Data/train_rebalancing_v3.csv').drop(columns=['ID'])
test = pd.read_csv('C:/Users/ANTL/Desktop/LG Aimers Data/test.csv').drop(columns=['ID'])

# -------------- ğŸ“Œ íƒ€ê²Ÿ ë³€ìˆ˜ ë¶„ë¦¬ -------------- 
X = train.drop(columns=['ì„ì‹  ì„±ê³µ ì—¬ë¶€'])  # ì…ë ¥ ë°ì´í„° (Feature)
y = train['ì„ì‹  ì„±ê³µ ì—¬ë¶€']  # íƒ€ê²Ÿ ë³€ìˆ˜ (Label)

# -------------- ğŸ“Œ Train-Test Split -------------- 
X_train, X_valid, y_train, y_valid = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"ğŸ”¹ í•™ìŠµ ë°ì´í„° í¬ê¸°: {X_train.shape}, ê²€ì¦ ë°ì´í„° í¬ê¸°: {X_valid.shape}")

# -------------- ğŸ“Œ ê²°ì¸¡ì¹˜ ì²˜ë¦¬ -------------- 
missing_percentage = (X_train.isnull().sum() / len(X_train)) * 100

# ê²°ì¸¡ì¹˜ ë¹„ìœ¨ì´ 50% ì´ìƒì¸ ì»¬ëŸ¼ ì‚­ì œ
high_missing_columns = missing_percentage[missing_percentage >= 50].index
X_train = X_train.drop(columns=high_missing_columns, errors='ignore')
X_valid = X_valid.drop(columns=high_missing_columns, errors='ignore')
test = test.drop(columns=high_missing_columns, errors='ignore')

# ê²°ì¸¡ì¹˜ ë¹„ìœ¨ì´ 0~10%ì¸ ì»¬ëŸ¼ì´ í¬í•¨ëœ í–‰ ì‚­ì œ
low_missing_columns = missing_percentage[(missing_percentage > 0) & (missing_percentage < 10)].index
if len(low_missing_columns) > 0:
    missing_rows = X_train[low_missing_columns].isnull().any(axis=1)
    X_train = X_train[~missing_rows]
    y_train = y_train[~missing_rows]  # ğŸ”¥ y_trainë„ ë™ì¼í•œ í–‰ ì‚­ì œ

# ğŸ”¥ ì¸ë±ìŠ¤ ë¦¬ì…‹ (Train-Test Split í›„ ë°ì´í„° ì •ë ¬ ë¬¸ì œ í•´ê²°)
X_train = X_train.reset_index(drop=True)
y_train = y_train.reset_index(drop=True)

# -------------- ğŸ“Œ ë²”ì£¼í˜• ì»¬ëŸ¼ ìë™ ê°ì§€ & ì¸ì½”ë”© -------------- 
categorical_columns = X_train.select_dtypes(include=['object']).columns.tolist()
print(f"ğŸ“Œ ê°ì§€ëœ ë²”ì£¼í˜• ì»¬ëŸ¼: {categorical_columns}")

# ğŸ”¥ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œë„ ë™ì¼í•œ ì»¬ëŸ¼ ìœ ì§€
test = test[X_train.columns]

# ë²”ì£¼í˜• ë°ì´í„°ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
for col in categorical_columns:
    X_train[col] = X_train[col].astype(str)
    X_valid[col] = X_valid[col].astype(str)
    test[col] = test[col].astype(str)

# OrdinalEncoder ì„¤ì •
ordinal_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)

# í•™ìŠµ ë°ì´í„° ì¸ì½”ë”©
X_train_encoded = X_train.copy()
X_train_encoded[categorical_columns] = ordinal_encoder.fit_transform(X_train[categorical_columns])

# ê²€ì¦ ë°ì´í„° & í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¸ì½”ë”©
X_valid_encoded = X_valid.copy()
X_valid_encoded[categorical_columns] = ordinal_encoder.transform(X_valid[categorical_columns])

X_test_encoded = test.copy()
X_test_encoded[categorical_columns] = ordinal_encoder.transform(test[categorical_columns])

# ğŸ”¥ ì»¬ëŸ¼ ì´ë¦„ì„ ìœ ì§€í•˜ë„ë¡ ë³€í™˜
X_test_encoded = pd.DataFrame(X_test_encoded, columns=X_train_encoded.columns)

# -------------- ğŸ“Œ ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ìë™ ê°ì§€ & ê²°ì¸¡ì¹˜ ì²˜ë¦¬ -------------- 
numeric_columns = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()
print(f"ğŸ“Œ ê°ì§€ëœ ìˆ˜ì¹˜í˜• ì»¬ëŸ¼: {numeric_columns}")

# NaNì„ 0ìœ¼ë¡œ ì±„ìš°ê¸°
X_train_encoded[numeric_columns] = X_train_encoded[numeric_columns].fillna(0)
X_valid_encoded[numeric_columns] = X_valid_encoded[numeric_columns].fillna(0)
X_test_encoded[numeric_columns] = X_test_encoded[numeric_columns].fillna(0)

# -------------- ğŸ“Œ ëª¨ë¸ í•™ìŠµ (XGBoost) -------------- 
model = XGBClassifier(
    n_estimators=300,   # íŠ¸ë¦¬ ê°œìˆ˜
    max_depth=6,        # íŠ¸ë¦¬ ìµœëŒ€ ê¹Šì´
    learning_rate=0.05,  # í•™ìŠµë¥  (ë‚®ì¶œìˆ˜ë¡ í•™ìŠµì´ ë” ì²œì²œíˆ ì§„í–‰ë¨)
    subsample=0.8,      # ë°ì´í„° ìƒ˜í”Œë§ ë¹„ìœ¨
    colsample_bytree=0.8,  # íŠ¸ë¦¬ë§ˆë‹¤ ì‚¬ìš©í•  Feature ë¹„ìœ¨
    random_state=42,
    use_label_encoder=False,  # ìµœì‹  XGBoost ë²„ì „ì—ì„œëŠ” Falseë¡œ ì„¤ì •
    eval_metric="logloss"  # í‰ê°€ ê¸°ì¤€
)

model.fit(X_train_encoded, y_train)

# -------------- ğŸ“Œ ê²€ì¦ ë°ì´í„°ì—ì„œ ROC-AUC ì„±ëŠ¥ í‰ê°€ -------------- 
valid_pred_proba = model.predict_proba(X_valid_encoded)[:, 1]  # ì„ì‹  ì„±ê³µ í™•ë¥  (1 í´ë˜ìŠ¤ í™•ë¥ )
auc_score = roc_auc_score(y_valid, valid_pred_proba)

print(f"ğŸ”¥ ê²€ì¦ ë°ì´í„° ROC-AUC Score: {auc_score:.4f}")

# -------------- ğŸ“Œ ê²€ì¦ ë°ì´í„°ì—ì„œ 0 ë˜ëŠ” 1 ì˜ˆì¸¡ (í™•ë¥  â†’ í´ë˜ìŠ¤ ë³€í™˜) -------------- 
valid_pred_class = model.predict(X_valid_encoded)  # ğŸ”¥ í™•ë¥ ì´ ì•„ë‹Œ 0 ë˜ëŠ” 1 ì˜ˆì¸¡
accuracy = accuracy_score(y_valid, valid_pred_class)

print(f"âœ… ê²€ì¦ ë°ì´í„° Accuracy: {accuracy:.4f}")

# -------------- ğŸ“Œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ (0 ë˜ëŠ” 1) ë° ê²°ê³¼ ì €ì¥ -------------- 
test_pred_class = model.predict(X_test_encoded)  # ğŸ”¥ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œë„ 0 ë˜ëŠ” 1ë¡œ ì˜ˆì¸¡

sample_submission = pd.read_csv('C:/Users/ANTL/Desktop/LG Aimers Data/sample_submission.csv')
sample_submission['probability'] = test_pred_class  # ğŸ”¥ 0 ë˜ëŠ” 1ë¡œ ì €ì¥
sample_submission.to_csv('C:/Users/ANTL/Desktop/LG Aimers Data/baseline_submit_xgboost.csv', index=False)

print("âœ… XGBoost ëª¨ë¸ í•™ìŠµ & ì˜ˆì¸¡ ì™„ë£Œ, ê²°ê³¼ ì €ì¥ë¨.")
