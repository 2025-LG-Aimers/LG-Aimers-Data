import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.preprocessing import OrdinalEncoder
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split
from sklearn.metrics import log_loss, roc_auc_score, accuracy_score

# -------------- ğŸ“Œ ê²€ì¦ ë°ì´í„° ì˜ˆì¸¡ ë° í‰ê°€


# -------------- ğŸ“Œ ë°ì´í„° ë¡œë”© --------------
train = pd.read_csv('C:/Users/mch2d/Desktop/LG-Aimers-Data-main/train.csv').drop(columns=['ID'])
test = pd.read_csv('C:/Users/mch2d/Desktop/LG-Aimers-Data-main/test.csv').drop(columns=['ID'])

# -------------- ğŸ“Œ íƒ€ê²Ÿ ë³€ìˆ˜ ë¶„ë¦¬ --------------
y = train['ì„ì‹  ì„±ê³µ ì—¬ë¶€']  # íƒ€ê²Ÿ ë³€ìˆ˜ (Label)
X = train.drop(columns=['ì„ì‹  ì„±ê³µ ì—¬ë¶€'])  # ì…ë ¥ ë°ì´í„° (Feature)

# -------------- ğŸ“Œ Feature Engineering --------------
# âœ… "ë°°ì•„ ì‚¬ìš©ë¥ " Feature ì¶”ê°€
if "ì´ì‹ëœ ë°°ì•„ ìˆ˜" in X.columns and "ì €ì¥ëœ ë°°ì•„ ìˆ˜" in X.columns:
    X["ë°°ì•„ ì‚¬ìš©ë¥ "] = X["ì´ì‹ëœ ë°°ì•„ ìˆ˜"] / (X["ì´ì‹ëœ ë°°ì•„ ìˆ˜"] + X["ì €ì¥ëœ ë°°ì•„ ìˆ˜"] + 1e-5)
    test["ë°°ì•„ ì‚¬ìš©ë¥ "] = test["ì´ì‹ëœ ë°°ì•„ ìˆ˜"] / (test["ì´ì‹ëœ ë°°ì•„ ìˆ˜"] + test["ì €ì¥ëœ ë°°ì•„ ìˆ˜"] + 1e-5)

# âœ… "ë¶€ë¶€ ì£¼ ë¶ˆì„ ì›ì¸" ë° "ë¶€ë¶€ ë¶€ ë¶ˆì„ ì›ì¸" Feature ì¶”ê°€
if "ë‚¨ì„± ì£¼ ë¶ˆì„ ì›ì¸" in X.columns and "ì—¬ì„± ì£¼ ë¶ˆì„ ì›ì¸" in X.columns:
    X["ë¶€ë¶€ ì£¼ ë¶ˆì„ ì›ì¸2"] = X["ë‚¨ì„± ì£¼ ë¶ˆì„ ì›ì¸"] | X["ì—¬ì„± ì£¼ ë¶ˆì„ ì›ì¸"]
    test["ë¶€ë¶€ ì£¼ ë¶ˆì„ ì›ì¸2"] = test["ë‚¨ì„± ì£¼ ë¶ˆì„ ì›ì¸"] | test["ì—¬ì„± ì£¼ ë¶ˆì„ ì›ì¸"]

if "ë‚¨ì„± ë¶€ ë¶ˆì„ ì›ì¸" in X.columns and "ì—¬ì„± ë¶€ ë¶ˆì„ ì›ì¸" in X.columns:
    X["ë¶€ë¶€ ë¶€ ë¶ˆì„ ì›ì¸2"] = X["ë‚¨ì„± ë¶€ ë¶ˆì„ ì›ì¸"] | X["ì—¬ì„± ë¶€ ë¶ˆì„ ì›ì¸"]
    test["ë¶€ë¶€ ë¶€ ë¶ˆì„ ì›ì¸2"] = test["ë‚¨ì„± ë¶€ ë¶ˆì„ ì›ì¸"] | test["ì—¬ì„± ë¶€ ë¶ˆì„ ì›ì¸"]

# âœ… "ë¯¸ì„¸ì£¼ì… ì„±ê³µë¥ " & "ë¯¸ì„¸ì£¼ì… ì´ì‹ìœ¨" Feature ì¶”ê°€
X["ë¯¸ì„¸ì£¼ì… ì„±ê³µë¥ "] = X["ë¯¸ì„¸ì£¼ì…ì—ì„œ ìƒì„±ëœ ë°°ì•„ ìˆ˜"] / (X["ë¯¸ì„¸ì£¼ì…ëœ ë‚œì ìˆ˜"] + 1e-5)
test["ë¯¸ì„¸ì£¼ì… ì„±ê³µë¥ "] = test["ë¯¸ì„¸ì£¼ì…ì—ì„œ ìƒì„±ëœ ë°°ì•„ ìˆ˜"] / (test["ë¯¸ì„¸ì£¼ì…ëœ ë‚œì ìˆ˜"] + 1e-5)

X["ë¯¸ì„¸ì£¼ì… ì´ì‹ìœ¨"] = X["ë¯¸ì„¸ì£¼ì… ë°°ì•„ ì´ì‹ ìˆ˜"] / (X["ë¯¸ì„¸ì£¼ì…ì—ì„œ ìƒì„±ëœ ë°°ì•„ ìˆ˜"] + 1e-5)
test["ë¯¸ì„¸ì£¼ì… ì´ì‹ìœ¨"] = test["ë¯¸ì„¸ì£¼ì… ë°°ì•„ ì´ì‹ ìˆ˜"] / (test["ë¯¸ì„¸ì£¼ì…ì—ì„œ ìƒì„±ëœ ë°°ì•„ ìˆ˜"] + 1e-5)

# âœ… ìˆ«ì ë³€í™˜ í•¨ìˆ˜ ì •ì˜ (íšŸìˆ˜ ë³€í™˜)
def convert_to_number(value):
    if isinstance(value, str):
        value = value.replace("íšŒ", "").strip()
        if "ì´ìƒ" in value:
            return 6  
        return int(value)
    return value

# âœ… "IVF ì„ì‹  íšŸìˆ˜", "DI ì„ì‹  íšŸìˆ˜" ë“± ìˆ«ìë¡œ ë³€í™˜
for col in ["IVF ì„ì‹  íšŸìˆ˜", "DI ì„ì‹  íšŸìˆ˜", "ì´ ì„ì‹  íšŸìˆ˜", "IVF ì¶œì‚° íšŸìˆ˜", "DI ì¶œì‚° íšŸìˆ˜", "ì´ ì¶œì‚° íšŸìˆ˜"]:
    X[col] = X[col].apply(convert_to_number)
    test[col] = test[col].apply(convert_to_number)

# âœ… IVF & DI ë¹„ìœ¨ Feature ì¶”ê°€
X["IVF_ì„ì‹ _ë¹„ìœ¨"] = X["IVF ì„ì‹  íšŸìˆ˜"] / (X["ì´ ì„ì‹  íšŸìˆ˜"] + 1e-5)
X["DI_ì„ì‹ _ë¹„ìœ¨"] = X["DI ì„ì‹  íšŸìˆ˜"] / (X["ì´ ì„ì‹  íšŸìˆ˜"] + 1e-5)
X["IVF_ì¶œì‚°_ë¹„ìœ¨"] = X["IVF ì¶œì‚° íšŸìˆ˜"] / (X["ì´ ì¶œì‚° íšŸìˆ˜"] + 1e-5)
X["DI_ì¶œì‚°_ë¹„ìœ¨"] = X["DI ì¶œì‚° íšŸìˆ˜"] / (X["ì´ ì¶œì‚° íšŸìˆ˜"] + 1e-5)

test["IVF_ì„ì‹ _ë¹„ìœ¨"] = test["IVF ì„ì‹  íšŸìˆ˜"] / (test["ì´ ì„ì‹  íšŸìˆ˜"] + 1e-5)
test["DI_ì„ì‹ _ë¹„ìœ¨"] = test["DI ì„ì‹  íšŸìˆ˜"] / (test["ì´ ì„ì‹  íšŸìˆ˜"] + 1e-5)
test["IVF_ì¶œì‚°_ë¹„ìœ¨"] = test["IVF ì¶œì‚° íšŸìˆ˜"] / (test["ì´ ì¶œì‚° íšŸìˆ˜"] + 1e-5)
test["DI_ì¶œì‚°_ë¹„ìœ¨"] = test["DI ì¶œì‚° íšŸìˆ˜"] / (test["ì´ ì¶œì‚° íšŸìˆ˜"] + 1e-5)

# -------------- ğŸ“Œ ê²°ì¸¡ì¹˜ ì²˜ë¦¬ --------------
missing_percentage = (X.isnull().sum() / len(X)) * 100

# âœ… 1. ê²°ì¸¡ì¹˜ ë¹„ìœ¨ì´ 50% ì´ìƒì¸ ì»¬ëŸ¼ ì‚­ì œ
high_missing_columns = missing_percentage[missing_percentage >= 50].index.tolist()
X.drop(columns=high_missing_columns, inplace=True, errors='ignore')
test.drop(columns=high_missing_columns, inplace=True, errors='ignore')

# âœ… 2. ê²°ì¸¡ì¹˜ ë¹„ìœ¨ì´ 15% ~ 30%ì¸ ì»¬ëŸ¼ ì²˜ë¦¬
mid_missing_columns = missing_percentage[(missing_percentage >= 15) & (missing_percentage < 30)].index.tolist()
imputer_numeric = SimpleImputer(strategy='mean')  # í‰ê· ê°’ ëŒ€ì²´
X[mid_missing_columns] = imputer_numeric.fit_transform(X[mid_missing_columns])
test[mid_missing_columns] = imputer_numeric.transform(test[mid_missing_columns])

# âœ… 3. ë²”ì£¼í˜• ì»¬ëŸ¼ ìµœë¹ˆê°’ ëŒ€ì²´ + ì¸ì½”ë”©
categorical_columns = X.select_dtypes(include=['object']).columns.tolist()
imputer_categorical = SimpleImputer(strategy='most_frequent')
X[categorical_columns] = imputer_categorical.fit_transform(X[categorical_columns])
test[categorical_columns] = imputer_categorical.transform(test[categorical_columns])

ordinal_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)
X[categorical_columns] = ordinal_encoder.fit_transform(X[categorical_columns])
test[categorical_columns] = ordinal_encoder.transform(test[categorical_columns])

# âœ… ìµœì¢… ë‚¨ì•„ìˆëŠ” ì»¬ëŸ¼ í™•ì¸
remaining_columns = X.columns.tolist()
print(f"\nâœ… ìµœì¢… ë‚¨ì•„ìˆëŠ” ì»¬ëŸ¼ ê°œìˆ˜: {len(remaining_columns)}")
print(f"âœ… ìµœì¢… ë‚¨ì•„ìˆëŠ” ì»¬ëŸ¼ ëª©ë¡: {remaining_columns}")


# -------------- ğŸ“Œ XGBoost ëª¨ë¸ í•™ìŠµ --------------
params = {
    "objective": "binary:logistic",
    "eval_metric": "logloss",
    "max_depth": 6,
    "learning_rate": 0.03,
    "subsample": 0.8,
    "colsample_bytree": 0.8,
    "random_state": 42
}

# -------------- ğŸ“Œ Gain í•˜ìœ„ 40% Feature ì œê±° --------------
dtrain_temp = xgb.DMatrix(X, label=y)

xgb_model_temp = xgb.train(
    params=params,  # âœ… paramsê°€ ì´ì œ ì •ìƒì ìœ¼ë¡œ ì •ì˜ë¨
    dtrain=dtrain_temp,
    num_boost_round=500,
    verbose_eval=False
)

feature_importances = xgb_model_temp.get_score(importance_type="gain")
importance_df = pd.DataFrame({
    "Feature": list(feature_importances.keys()),
    "Gain": list(feature_importances.values())
}).sort_values(by="Gain", ascending=False)

# âœ… í•˜ìœ„ 40% Feature íƒìƒ‰
gain_threshold = np.percentile(importance_df["Gain"], 40)
low_gain_features = importance_df[importance_df["Gain"] <= gain_threshold]["Feature"].tolist()

# âœ… í•˜ìœ„ 40% Feature ì œê±°
X.drop(columns=low_gain_features, inplace=True, errors='ignore')
test.drop(columns=low_gain_features, inplace=True, errors='ignore')

print(f"\nğŸš€ Gain í•˜ìœ„ 40% (â‰¤ {gain_threshold:.2f}) Feature ê°œìˆ˜: {len(low_gain_features)}")
print(f"ğŸ“Œ ì œê±°ëœ Feature ëª©ë¡: {low_gain_features}")

# âœ… ì»¬ëŸ¼ ì‚­ì œ ì ìš© (train & test)
columns_to_remove = [
    "ì‹ ì„  ë°°ì•„ ì‚¬ìš© ì—¬ë¶€", "ë¯¸ì„¸ì£¼ì…ëœ ë‚œì ìˆ˜", "í˜¼í•©ëœ ë‚œì ìˆ˜",
    "ì´ ì‹œìˆ  íšŸìˆ˜", "ì´ ì„ì‹  íšŸìˆ˜", "ì´ ì¶œì‚° íšŸìˆ˜", "ì •ì ì¶œì²˜",
    "IVF ì„ì‹  íšŸìˆ˜", "IVF ì¶œì‚° íšŸìˆ˜", "DI ì¶œì‚° íšŸìˆ˜",
    "ë¶€ë¶€ ë¶€ ë¶ˆì„ ì›ì¸", "ë‚¨ì„± ë¶€ ë¶ˆì„ ì›ì¸",
    "ë¶ˆì„ ì›ì¸ - ë‚œê´€ ì§ˆí™˜", "ë¶ˆì„ ì›ì¸ - ë‚¨ì„± ìš”ì¸", "ë¶ˆì„ ì›ì¸ - ì—¬ì„± ìš”ì¸", "ë¶ˆì„ ì›ì¸ - ìê¶ê²½ë¶€ ë¬¸ì œ", "ë¶ˆì„ ì›ì¸ - ì •ì ë©´ì—­í•™ì  ìš”ì¸",
    "ì‹œìˆ  ìœ í˜•"
]
X.drop(columns=columns_to_remove, inplace=True, errors='ignore')
test.drop(columns=columns_to_remove, inplace=True, errors='ignore')


# âœ… ìµœì¢… ë‚¨ì•„ìˆëŠ” ì»¬ëŸ¼ í™•ì¸
remaining_columns = X.columns.tolist()
print(f"\nâœ… Gain í•˜ìœ„ 40% ì œê±° í›„ ë‚¨ì•„ìˆëŠ” ì»¬ëŸ¼ ê°œìˆ˜: {len(remaining_columns)}")
print(f"âœ… Gain í•˜ìœ„ 40% ì œê±° í›„ ë‚¨ì•„ìˆëŠ” ì»¬ëŸ¼ ëª©ë¡: {remaining_columns}")

# -------------- ğŸ“Œ ğŸ”¥ Train-Test Split (8:2) --------------
X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# âœ… XGBoost ì „ìš© DMatrix ìƒì„±
dtrain = xgb.DMatrix(X_train, label=y_train)
dvalid = xgb.DMatrix(X_valid, label=y_valid)
dtest = xgb.DMatrix(test)

# -------------- ğŸ“Œ XGBoost ëª¨ë¸ í•™ìŠµ --------------
params = {
    "objective": "binary:logistic",
    "eval_metric": "logloss",
    "max_depth": 6,
    "learning_rate": 0.03,
    "subsample": 0.8,
    "colsample_bytree": 0.8,
    "random_state": 42
}

# âœ… XGBoost ì „ìš© DMatrix ìƒì„±
dtrain = xgb.DMatrix(X, label=y)  # âœ… ì „ì²´ ë°ì´í„°ë¥¼ í•™ìŠµì— ì‚¬ìš©

# âœ… ëª¨ë¸ í•™ìŠµ (ì¡°ê¸° ì¢…ë£Œ ì—†ìŒ)
xgb_model = xgb.train(
    params=params,
    dtrain=dtrain,
    num_boost_round=500,  # âœ… Early Stopping ì œê±°í–ˆìœ¼ë¯€ë¡œ 500ë²ˆ í•™ìŠµ
    verbose_eval=True
)

# # âœ… ì¡°ê¸° ì¢…ë£Œ(Early Stopping) ì¶”ê°€
# xgb_model = xgb.train(
#     params=params,
#     dtrain=dtrain,
#     num_boost_round=500,
#     evals=[(dtrain, "train"), (dvalid, "valid")],
#     early_stopping_rounds=50,  # 50ë¼ìš´ë“œ ë™ì•ˆ ê°œì„  ì—†ìœ¼ë©´ ë©ˆì¶¤
#     verbose_eval=50
# )

# # -------------- ğŸ“Œ ê²€ì¦ ë°ì´í„° ì˜ˆì¸¡ ë° í‰ê°€ --------------
# valid_pred_proba = xgb_model.predict(dvalid)  # í™•ë¥ ê°’ ì˜ˆì¸¡
# valid_pred_labels = (valid_pred_proba >= 0.5).astype(int)  # 0.5 ê¸°ì¤€ìœ¼ë¡œ ì´ì§„ ë¶„ë¥˜

# # âœ… LogLoss Score
# logloss_score = log_loss(y_valid, valid_pred_proba)

# # âœ… ROC-AUC Score
# roc_auc = roc_auc_score(y_valid, valid_pred_proba)

# # âœ… Accuracy Score
# accuracy = accuracy_score(y_valid, valid_pred_labels)

# # -------------- ğŸ“Œ í‰ê°€ ê²°ê³¼ ì¶œë ¥ --------------
# print(f"\nâœ… ê²€ì¦ ë°ì´í„° LogLoss Score: {logloss_score:.5f}")
# print(f"âœ… ê²€ì¦ ë°ì´í„° ROC-AUC Score: {roc_auc:.5f}")
# print(f"âœ… ê²€ì¦ ë°ì´í„° Accuracy Score: {accuracy:.5f}")


# -------------- ğŸ“Œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ ë° ê²°ê³¼ ì €ì¥ --------------
dtest = xgb.DMatrix(test)
test_pred_proba = xgb_model.predict(dtest)

sample_submission = pd.read_csv('C:/Users/mch2d/Desktop/LG-Aimers-Data-main/sample_submission.csv')
sample_submission['probability'] = test_pred_proba
sample_submission.to_csv('C:/Users/mch2d/Desktop/LG-Aimers-Data-main/XGBoost_full_Original_HighScore_v12.csv', index=False)

print("\nâœ… XGBoost ëª¨ë¸ í•™ìŠµ & ì˜ˆì¸¡ ì™„ë£Œ, ê²°ê³¼ ì €ì¥ë¨.")
