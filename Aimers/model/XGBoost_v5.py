# ì „ì²´ ë°ì´í„°(train.csv)ë¥¼ 100% í•™ìŠµì— ì‚¬ìš©í•˜ëŠ” XGBoost ì½”ë“œ

import pandas as pd
import numpy as np
import xgboost as xgb  # âœ… XGBoost ì „ì²´ ëª¨ë“ˆ ì‚¬ìš©
from sklearn.preprocessing import OrdinalEncoder
from sklearn.impute import SimpleImputer  # âœ… ê²°ì¸¡ì¹˜ í‰ê·  ëŒ€ì²´ ì¶”ê°€
from sklearn.decomposition import PCA

# -------------- ğŸ“Œ ë°ì´í„° ë¡œë”© --------------
train = pd.read_csv('C:/Users/ANTL/Documents/GitHub/LG-Aimers-Data/train.csv').drop(columns=['ID'])
test = pd.read_csv('C:/Users/ANTL/Documents/GitHub/LG-Aimers-Data/test.csv').drop(columns=['ID'])

# -------------- ğŸ“Œ íƒ€ê²Ÿ ë³€ìˆ˜ ë¶„ë¦¬ --------------
X = train.drop(columns=['ì„ì‹  ì„±ê³µ ì—¬ë¶€'])  # ì…ë ¥ ë°ì´í„° (Feature)
y = train['ì„ì‹  ì„±ê³µ ì—¬ë¶€']  # íƒ€ê²Ÿ ë³€ìˆ˜ (Label)

# ì œê±°í•  ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
columns_to_remove = ["ì‹ ì„  ë°°ì•„ ì‚¬ìš© ì—¬ë¶€", "ë¯¸ì„¸ì£¼ì…ëœ ë‚œì ìˆ˜", "í˜¼í•©ëœ ë‚œì ìˆ˜"]

# # âœ… PCA ì ìš©í•  ë³€ìˆ˜ ì„ íƒ
# pca_features = ["ë¯¸ì„¸ì£¼ì…ëœ ë‚œì ìˆ˜", "ë¯¸ì„¸ì£¼ì…ì—ì„œ ìƒì„±ëœ ë°°ì•„ ìˆ˜"]

# # âœ… NaNì„ í‰ê· ê°’ìœ¼ë¡œ ëŒ€ì²´
# X[pca_features] = X[pca_features].fillna(X[pca_features].mean())

# # âœ… PCA ë³€í™˜ ìˆ˜í–‰
# pca = PCA(n_components=1)
# X_pca = pca.fit_transform(X[pca_features])

# # âœ… PCA ë³€ìˆ˜ë¥¼ ì¶”ê°€í•œ í›„ ì›ë³¸ ë³€ìˆ˜ ì œê±°
# X["PCA_ë°°ì•„_ë‚œì"] = X_pca
# test[pca_features] = test[pca_features].fillna(test[pca_features].mean())
# test_pca = pca.transform(test[pca_features])  # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë³€í™˜
# test["PCA_ë°°ì•„_ë‚œì"] = test_pca

# X.drop(columns=pca_features, inplace=True, errors='ignore')
# test.drop(columns=pca_features, inplace=True, errors='ignore')

# print(f"âœ… PCA ì ìš© ì™„ë£Œ! ë³€í™˜ëœ ë³€ìˆ˜: PCA_ë°°ì•„_ë‚œì")

# í¸í–¥ëœ ì»¬ëŸ¼ íƒìƒ‰ (95% ì´ìƒ í•œ ê°’ìœ¼ë¡œ ì±„ì›Œì§„ ì»¬ëŸ¼)
threshold = 0.95
biased_columns = [col for col in X.columns if X[col].value_counts(normalize=True).max() >= threshold]

# ì „ì²´ ì œê±°í•  ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
columns_to_remove.extend(biased_columns)

# ì»¬ëŸ¼ ì‚­ì œ ì ìš©
X.drop(columns=columns_to_remove, inplace=True, errors='ignore')
test.drop(columns=columns_to_remove, inplace=True, errors='ignore')

print(f"âœ… ì œê±°ëœ ì»¬ëŸ¼: {columns_to_remove}")

# -------------- ğŸ“Œ ê²°ì¸¡ì¹˜ ì²˜ë¦¬ --------------
missing_percentage = (X.isnull().sum() / len(X)) * 100

# âœ… 1. ê²°ì¸¡ì¹˜ ë¹„ìœ¨ì´ 80% ì´ìƒì¸ ì»¬ëŸ¼ ì‚­ì œ
high_missing_columns = missing_percentage[missing_percentage >= 80].index
X.drop(columns=high_missing_columns, inplace=True, errors='ignore')
test.drop(columns=high_missing_columns, inplace=True, errors='ignore')

# âœ… 2. ê²°ì¸¡ì¹˜ ë¹„ìœ¨ì´ 15% ~ 30%ì¸ ì»¬ëŸ¼ í‰ê· ê°’ìœ¼ë¡œ ëŒ€ì²´
mid_missing_columns = missing_percentage[(missing_percentage >= 15) & (missing_percentage < 30)].index
imputer = SimpleImputer(strategy='mean')  # í‰ê· ê°’ ëŒ€ì²´
X[mid_missing_columns] = imputer.fit_transform(X[mid_missing_columns])
test[mid_missing_columns] = imputer.transform(test[mid_missing_columns])

# âœ… 3. ë²”ì£¼í˜• ì»¬ëŸ¼ ìë™ ê°ì§€ & ì¸ì½”ë”© (Ordinal Encoding)
categorical_columns = X.select_dtypes(include=['object']).columns.tolist()
print(f"ğŸ“Œ ê°ì§€ëœ ë²”ì£¼í˜• ì»¬ëŸ¼: {categorical_columns}")

# ğŸ”¥ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œë„ ë™ì¼í•œ ì»¬ëŸ¼ ìœ ì§€
test = test[X.columns]

# ë²”ì£¼í˜• ë°ì´í„°ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
for col in categorical_columns:
    X[col] = X[col].astype(str)
    test[col] = test[col].astype(str)

# âœ… OrdinalEncoder ì„¤ì • ë° ë³€í™˜
ordinal_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)
X[categorical_columns] = ordinal_encoder.fit_transform(X[categorical_columns])
test[categorical_columns] = ordinal_encoder.transform(test[categorical_columns])

# -------------- ğŸ“Œ ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ìë™ ê°ì§€ & ê²°ì¸¡ì¹˜ ì²˜ë¦¬ --------------
numeric_columns = X.select_dtypes(include=['int64', 'float64']).columns.tolist()
print(f"ğŸ“Œ ê°ì§€ëœ ìˆ˜ì¹˜í˜• ì»¬ëŸ¼: {numeric_columns}")

# NaNì„ 0ìœ¼ë¡œ ì±„ìš°ê¸° (ì¶”ê°€ì ì¸ ê²°ì¸¡ì¹˜ ì²˜ë¦¬)
X[numeric_columns] = X[numeric_columns].fillna(0)
test[numeric_columns] = test[numeric_columns].fillna(0)

# -------------- ğŸ“Œ XGBoost ëª¨ë¸ í•™ìŠµ --------------
params = {
    "objective": "binary:logistic",
    "eval_metric": "logloss",
    "max_depth": 6,
    "learning_rate": 0.03,
    "subsample": 0.8,
    "colsample_bytree": 0.8,
    "random_state": 42
}

# XGBoost ì „ìš© DMatrix ìƒì„±
dtrain = xgb.DMatrix(X, label=y)  # âœ… ì „ì²´ ë°ì´í„°ë¥¼ í•™ìŠµì— ì‚¬ìš©

# ğŸ”¥ ëª¨ë¸ í•™ìŠµ (ì¡°ê¸° ì¢…ë£Œ ì—†ìŒ)
xgb_model = xgb.train(
    params=params,
    dtrain=dtrain,
    num_boost_round=500,  # âœ… Early Stopping ì œê±°í–ˆìœ¼ë¯€ë¡œ 500ë²ˆ í•™ìŠµ
    verbose_eval=True
)

# -------------- ğŸ“Œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ ë° ê²°ê³¼ ì €ì¥ --------------
dtest = xgb.DMatrix(test)
test_pred_proba = xgb_model.predict(dtest)

sample_submission = pd.read_csv('C:/Users/ANTL/Documents/GitHub/LG-Aimers-Data/sample_submission.csv')
sample_submission['probability'] = test_pred_proba
sample_submission.to_csv('C:/Users/ANTL/Documents/GitHub/LG-Aimers-Data/XGBoost_full_train(threshold_0.95).csv', index=False)

print("âœ… XGBoost ëª¨ë¸ í•™ìŠµ & ì˜ˆì¸¡ ì™„ë£Œ, ê²°ê³¼ ì €ì¥ë¨.")