import optuna
import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import OrdinalEncoder
from sklearn.impute import SimpleImputer
from sklearn.metrics import roc_auc_score

# -------------- ğŸ“Œ ë°ì´í„° ë¡œë”© --------------
train = pd.read_csv('C:/Users/ANTL/Documents/GitHub/LG-Aimers-Data/train.csv').drop(columns=['ID'])
test = pd.read_csv('C:/Users/ANTL/Documents/GitHub/LG-Aimers-Data/test.csv').drop(columns=['ID'])

# -------------- ğŸ“Œ íƒ€ê²Ÿ ë³€ìˆ˜ ë¶„ë¦¬ --------------
X = train.drop(columns=['ì„ì‹  ì„±ê³µ ì—¬ë¶€'])
y = train['ì„ì‹  ì„±ê³µ ì—¬ë¶€']

# âœ… í¸í–¥ëœ ì»¬ëŸ¼ ì œê±°
columns_to_remove = ["ì‹ ì„  ë°°ì•„ ì‚¬ìš© ì—¬ë¶€", "ë¯¸ì„¸ì£¼ì…ëœ ë‚œì ìˆ˜", "IVF ì‹œìˆ  íšŸìˆ˜", 
                     "IVF ì„ì‹  íšŸìˆ˜", "ì´ ì¶œì‚° íšŸìˆ˜", "ì •ì ì¶œì²˜"]

# âœ… 95% ì´ìƒ í¸í–¥ëœ ì»¬ëŸ¼ ì œê±°
threshold = 0.95
biased_columns = [col for col in X.columns if X[col].value_counts(normalize=True).max() >= threshold]
columns_to_remove.extend(biased_columns)

X.drop(columns=columns_to_remove, inplace=True, errors='ignore')
test.drop(columns=columns_to_remove, inplace=True, errors='ignore')

# -------------- ğŸ“Œ ê²°ì¸¡ì¹˜ ì²˜ë¦¬ --------------
missing_percentage = (X.isnull().sum() / len(X)) * 100

# âœ… 80% ì´ìƒ ê²°ì¸¡ì¹˜ ì»¬ëŸ¼ ì‚­ì œ
high_missing_columns = missing_percentage[missing_percentage >= 80].index.tolist()
X.drop(columns=high_missing_columns, inplace=True, errors='ignore')
test.drop(columns=high_missing_columns, inplace=True, errors='ignore')

# âœ… 15% ~ 30% ê²°ì¸¡ì¹˜ í‰ê· ê°’ìœ¼ë¡œ ëŒ€ì²´
mid_missing_columns = missing_percentage[(missing_percentage >= 15) & (missing_percentage < 30)].index.tolist()
imputer = SimpleImputer(strategy='mean')
X[mid_missing_columns] = imputer.fit_transform(X[mid_missing_columns])
test[mid_missing_columns] = imputer.transform(test[mid_missing_columns])

# âœ… ë²”ì£¼í˜• ì»¬ëŸ¼ ì¸ì½”ë”© (Ordinal Encoding)
categorical_columns = X.select_dtypes(include=['object']).columns.tolist()

# ğŸ”¥ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œë„ ë™ì¼í•œ ì»¬ëŸ¼ ìœ ì§€
test = test[X.columns]

# ë²”ì£¼í˜• ë°ì´í„°ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜ í›„ ì¸ì½”ë”©
for col in categorical_columns:
    X[col] = X[col].astype(str)
    test[col] = test[col].astype(str)

ordinal_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)
X[categorical_columns] = ordinal_encoder.fit_transform(X[categorical_columns])
test[categorical_columns] = ordinal_encoder.transform(test[categorical_columns])

# âœ… ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ NaN ì±„ìš°ê¸°
numeric_columns = X.select_dtypes(include=['int64', 'float64']).columns.tolist()
X[numeric_columns] = X[numeric_columns].fillna(0)
test[numeric_columns] = test[numeric_columns].fillna(0)

# -------------- ğŸ“Œ Focal Loss ì •ì˜ --------------
def focal_loss(predt, dtrain, gamma, alpha):
    """ Focal Loss ì»¤ìŠ¤í…€ í•¨ìˆ˜ (Gradient & Hessian ê³„ì‚°) """
    y = dtrain.get_label()
    p = 1 / (1 + np.exp(-predt))  # ì‹œê·¸ëª¨ì´ë“œ ë³€í™˜
    grad = alpha * (1 - p) ** gamma * (p - y)
    hess = alpha * (1 - p) ** gamma * p * (1 - p) * (1 + gamma * (1 - p) * (y - p))
    return grad, hess

# -------------- ğŸ“Œ Optuna ìµœì í™” í•¨ìˆ˜ --------------
def objective(trial):
    """ Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” í•¨ìˆ˜ """
    # âœ… ìµœì í™”í•  í•˜ì´í¼íŒŒë¼ë¯¸í„° ë²”ìœ„ ì„¤ì •
    max_depth = trial.suggest_int("max_depth", 3, 10)
    learning_rate = trial.suggest_float("learning_rate", 0.005, 0.05, log=True)
    gamma = trial.suggest_float("gamma", 0.5, 3.0)
    alpha = trial.suggest_float("alpha", 0.1, 0.5)

    # âœ… XGBoost íŒŒë¼ë¯¸í„° ì„¤ì •
    params = {
        "objective": "binary:logistic",
        "eval_metric": "logloss",
        "max_depth": max_depth,
        "learning_rate": learning_rate,
        "subsample": 0.8,
        "colsample_bytree": 0.8,
        "random_state": 42
    }

    # âœ… 5-Fold Stratified K-Fold ì„¤ì •
    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    auc_scores = []

    # âœ… Cross Validation ìˆ˜í–‰
    for train_idx, valid_idx in skf.split(X, y):
        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]
        y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]

        dtrain = xgb.DMatrix(X_train, label=y_train)
        dvalid = xgb.DMatrix(X_valid, label=y_valid)

        # âœ… XGBoost í•™ìŠµ (Focal Loss ì ìš©)
        model = xgb.train(
            params=params,
            dtrain=dtrain,
            num_boost_round=500,
            obj=lambda predt, dtrain: focal_loss(predt, dtrain, gamma, alpha),
            evals=[(dvalid, "valid")],
            early_stopping_rounds=50,
            verbose_eval=False
        )

        # âœ… ê²€ì¦ ë°ì´í„° ì˜ˆì¸¡ ë° ì„±ëŠ¥ ì¸¡ì •
        valid_pred_proba = model.predict(dvalid)
        auc = roc_auc_score(y_valid, valid_pred_proba)
        auc_scores.append(auc)

    return np.mean(auc_scores)  # âœ… í‰ê·  AUC ë°˜í™˜ (ìµœëŒ€í™” ëª©í‘œ)

# -------------- ğŸ“Œ Optuna ì‹¤í–‰ --------------
study = optuna.create_study(direction="maximize")  # AUC ìµœëŒ€í™”
study.optimize(objective, n_trials=50)  # 50ë²ˆ ìµœì í™” ì‹œë„

# âœ… ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¶œë ¥
print("\nğŸ”¥ ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° ğŸ”¥")
print(study.best_params)
print(f"âœ… ìµœê³  AUC Score: {study.best_value:.6f}")
