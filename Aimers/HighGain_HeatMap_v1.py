import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import xgboost as xgb  # âœ… XGBoost ì‚¬ìš©
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OrdinalEncoder
from sklearn.impute import SimpleImputer

# âœ… Matplotlib í•œê¸€ í°íŠ¸ ì„¤ì • (Windows: 'Malgun Gothic', Mac: 'AppleGothic')
plt.rc('font', family='Malgun Gothic')
plt.rcParams['axes.unicode_minus'] = False  # ë§ˆì´ë„ˆìŠ¤ ê¸°í˜¸ ê¹¨ì§ ë°©ì§€

# âœ… ë°ì´í„° ë¡œë”©
train = pd.read_csv('C:/Users/ANTL/Documents/GitHub/LG-Aimers-Data/train.csv', encoding='utf-8')

# âœ… íƒ€ê²Ÿ ë³€ìˆ˜ ë¶„ë¦¬
X = train.drop(columns=['ì„ì‹  ì„±ê³µ ì—¬ë¶€'])
y = train['ì„ì‹  ì„±ê³µ ì—¬ë¶€']

# âœ… 1. ìˆ˜ì¹˜í˜• & ë²”ì£¼í˜• ë°ì´í„° ë¶„ë¦¬
numeric_columns = X.select_dtypes(include=['int64', 'float64']).columns.tolist()
categorical_columns = X.select_dtypes(include=['object']).columns.tolist()

# âœ… 2. ìˆ˜ì¹˜í˜• ë°ì´í„° ê²°ì¸¡ì¹˜ ì²˜ë¦¬ (í‰ê·  ëŒ€ì²´)
num_imputer = SimpleImputer(strategy='mean')
X[numeric_columns] = num_imputer.fit_transform(X[numeric_columns])

# âœ… 3. ë²”ì£¼í˜• ë°ì´í„° ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ("missing"ìœ¼ë¡œ ëŒ€ì²´)
X[categorical_columns] = X[categorical_columns].fillna("missing")

# âœ… 4. ë²”ì£¼í˜• ë°ì´í„° Ordinal Encoding
ordinal_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)
X[categorical_columns] = ordinal_encoder.fit_transform(X[categorical_columns])

# âœ… Train-Test Split (Feature Importance ê³„ì‚°ìš©)
X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# âœ… 5. XGBoost ëª¨ë¸ í•™ìŠµ (Feature Importance êµ¬í•˜ê¸° ìœ„í•¨)
params = {
    "objective": "binary:logistic",
    "eval_metric": "logloss",
    "max_depth": 5,
    "learning_rate": 0.03,
    "subsample": 0.8,
    "colsample_bytree": 0.83,
    "random_state": 42
}

dtrain = xgb.DMatrix(X_train, label=y_train)
dvalid = xgb.DMatrix(X_valid, label=y_valid)

xgb_model = xgb.train(
    params=params,
    dtrain=dtrain,
    num_boost_round=500,
    evals=[(dtrain, "train"), (dvalid, "valid")],
    verbose_eval=False
)

# âœ… 6. Feature Importance (Gain ê¸°ì¤€) ì €ì¥
importances_gain = xgb_model.get_score(importance_type='gain')
importance_df = pd.DataFrame({'Feature': list(importances_gain.keys()), 'Gain': list(importances_gain.values())})
importance_df = importance_df.sort_values(by='Gain', ascending=False)

# âœ… Feature Importance CSV ì €ì¥
importance_csv_path = 'C:/Users/ANTL/Documents/GitHub/LG-Aimers-Data/feature_importance.csv'
importance_df.to_csv(importance_csv_path, index=False)
print(f"âœ… Feature Importance ì €ì¥ ì™„ë£Œ: {importance_csv_path}")

# âœ… 7. Gainì´ ë†’ì€ ìƒìœ„ 10ê°œ Feature ì„ íƒ
top_features = importance_df.head(20)['Feature'].tolist()

# âœ… ì„ íƒëœ Featureë“¤ì˜ ìƒê´€í–‰ë ¬ ê³„ì‚°
X_selected = X_train[top_features]
corr_matrix = X_selected.corr()

# âœ… 8. ìƒì‚¼ê° í–‰ë ¬(Upper Triangle) ì œê±° (ë°˜ë§Œ í‘œì‹œ)
mask = np.triu(np.ones_like(corr_matrix, dtype=bool))

# ğŸ”¥ Heatmap ì‹œê°í™”
plt.figure(figsize=(12, 10))  # âœ… ê·¸ë˜í”„ í¬ê¸° ì¡°ì • (ë„ˆë¹„=12, ë†’ì´=10)
sns.heatmap(
    corr_matrix, mask=mask, annot=True, cmap="coolwarm", fmt=".2f", linewidths=0.5,
    annot_kws={"size": 8}  # âœ… ìˆ«ì í¬ê¸° ì¡°ì ˆ (ê¸°ë³¸: 10~12, ì¤„ì´ë ¤ë©´ 8~9 ì¶”ì²œ)
)
plt.xticks(rotation=45, ha='right', fontsize=10)  # âœ… Xì¶• ê¸€ì 45ë„ íšŒì „ + ì˜¤ë¥¸ìª½ ì •ë ¬
plt.yticks(fontsize=10)  # âœ… Yì¶• í°íŠ¸ í¬ê¸° ì¡°ì ˆ
plt.title("Gain ë†’ì€ Feature ê°„ ìƒê´€ê´€ê³„ Heatmap", fontsize=14)  # âœ… ì œëª© í¬ê¸° ì¡°ì ˆ
plt.subplots_adjust(bottom=0.3)  # âœ… Xì¶• ê¸€ìê°€ ë„ˆë¬´ ì•„ë˜ë¡œ ë‚´ë ¤ê°€ë©´ ì—¬ë°± ì¶”ê°€
plt.show()

